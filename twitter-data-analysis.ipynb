{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv',encoding=\"utf-8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/test.csv',encoding=\"utf-8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.figure(figsize=(12,8))\nsns.countplot(x='label',data=train,hue='label')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['number_words']=train['tweet'].apply(lambda x:len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['number_words']=test['tweet'].apply(lambda x:len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['number_words'],bins=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\np1=sns.kdeplot(train[train['label']==0]['number_words'],shade=True,color='b')\np2=sns.kdeplot(train[train['label']==1]['number_words'],shade=True,color='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['tweet']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nlemma = nltk.WordNetLemmatizer()\nstopwords=nltk.corpus.stopwords.words('english')\nimport re\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string.punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleantext(x):\n    tweets = \" \".join(filter(lambda x: x[0]!= '@' , x.split()))\n    tweets = re.sub('[^a-zA-Z]', ' ', tweets)\n    tweets = tweets.lower()\n    tweets = tweets.split()\n    tweets = [lemma.lemmatize(word) for word in tweets]\n    tweets = \" \".join(tweets)\n    return tweets\ndef remove_stopword(x):\n    tokens=re.split('\\W+',x)\n   #removal of stopwords from the tokenixed text\n    text=[lemma.lemmatize(word) for word in tokens if word not in stopwords]\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['clean_data']=train['tweet'].apply(lambda x:cleantext(x))\ntest['clean_data']=test['tweet'].apply(lambda x:cleantext(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['clean_data'][2:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['clean_text']=train['clean_data'].apply(lambda x:remove_stopword(x))\ntest['clean_text']=test['clean_data'].apply(lambda x:remove_stopword(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\ntop=Counter([item for sublist in train['clean_text'] for item in sublist])\ndf=pd.DataFrame(top.most_common(20))\ndf=df.iloc[1:,:]\ndf.columns=['Common_words','count']\ndf.style.background_gradient(cmap='OrRd')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure=px.treemap(df,path=['Common_words'],values='count')\nfigure.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most common words sentiment wise\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Positive_sent=train[train['label']==0]\nRacist_sent=train[train['label']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top=Counter([item for sublist in Positive_sent['clean_text'] for item in sublist])\ndf_pos=pd.DataFrame(top.most_common(20))\ndf_pos=df_pos.iloc[1:,:]\ndf_pos.columns=['Common_words','count']\ndf_pos.style.background_gradient(cmap='OrRd')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top=Counter([item for sublist in Racist_sent['clean_text'] for item in sublist])\ndf_racist=pd.DataFrame(top.most_common(20))\ndf_racist=df_racist.iloc[1:,:]\ndf_racist.columns=['Common_words','count']\ndf_racist.style.background_gradient(cmap='Greens')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure=px.treemap(df_racist,path=['Common_words'],values='count')\nfigure.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets try to print WORD-CLOUD\n=="},{"metadata":{"trusted":true},"cell_type":"code","source":"word_data_race=Racist_sent['clean_text']\nword_data_race[1:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def words(data):\n    all_words=[]\n    for text in data:\n        text=[x.strip(string.punctuation) for x in text]\n        all_words.append(text)\n\n    final=[\" \".join(text)for text in all_words]\n    final_data=\" \".join(final)\n    return final_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_racist=words(word_data_race)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_cloud(x):\n    #function to print word cloud\n    wordcloud = WordCloud(background_color=\"black\").generate(x)\n    plt.figure(figsize = (10,7))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_cloud(final_racist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_data_pos=Positive_sent['clean_text']\nword_data_pos[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pos=words(word_data_pos)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_cloud(final_pos)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recurrent neural network\n===="},{"metadata":{"trusted":true},"cell_type":"code","source":"# training the tokenizer and use tokenizer to convert the sentences to sequences of numbers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train['clean_data'])\nX_train_seq = tokenizer.texts_to_sequences(train['clean_data'])\nX_test_seq = tokenizer.texts_to_sequences(test['clean_data'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seq_padded = pad_sequences(X_train_seq, 50)\ntest_seq_padded = pad_sequences(X_test_seq, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.DataFrame(train['number_words'])\ndf2=pd.DataFrame(test['number_words'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\ntrain_rnn= hstack((train_seq_padded,df1))\ntest_rnn= hstack((test_seq_padded,df2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_rnn.shape,test_rnn.shape)\ny_train=train['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokenizer.index_word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.models import Sequential\n#instantiating our neural network model\nmodel = Sequential()\n\nmodel.add(Embedding(len(tokenizer.index_word)+1, 50))\nmodel.add(LSTM(32, dropout=0, recurrent_dropout=0))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=train_rnn.toarray()\ntest_data=test_rnn.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',loss='binary_crossentropy',\n              metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_data,\n                    y_train,\n                    batch_size=50, \n                    epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Loss')\nplt.plot(history.history['loss'],label='train_loss')\nplt.plot(history.history['accuracy'],label='train_accuracy')\nplt.plot(history.history['precision'],label='train_precision')\nplt.plot(history.history['recall'],label='train_precision')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install h5py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('Rnn_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By the end of 10 th epoch we got the below results:\nEpoch 10/10\nloss: 0.0032 - accuracy: 0.9991 - precision: 0.9951 - recall: 0.9924"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score\ny_pred = model.predict(test_data,verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Predicted labels_rnn']=pd.DataFrame(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random forest classifier\n====="},{"metadata":{},"cell_type":"markdown","source":"TF-IDF vectoriztion\n======"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer(ngram_range=(1, 2),min_df=2,max_features=1000)\ntfidf.fit(train['clean_data'])\ntfidf_df=tfidf.transform(train['clean_data']).toarray()\nprint('shape',tfidf_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_test=tfidf.transform(test['clean_data']).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train=train['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\n# with the same hstack function we are concatinating a sparse matrix and a dense matirx :)\ntrain_feat= hstack((tfidf_df,df1))\ntest_feat=hstack((tfidf_test,df2))\nprint(train_feat.shape)\nprint(test_feat.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\ndef gridcv(train_feat,test_feat,y_train):\n    parameters = {'max_depth': [1,2,3,4,5,6,7,8,9,10]}\n    model= GridSearchCV(RandomForestClassifier(), param_grid=parameters, n_jobs=-1,scoring='accuracy',cv=5)\n    model.fit(train_feat, y_train)\n    print('Best Estimator:      ',model.best_estimator_)\n    print('Optimal parameters:  ',model.best_params_)\n    print('optimal score:       ',model.best_score_*(100))\n    print('-------------------------------------------------')\n\n    model.best_estimator_.fit(train_feat, y_train)\n    y_pred = model.best_estimator_.predict(test_feat)\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y=gridcv(tfidf_df,tfidf_test,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['predicted_y']=pred_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['predicted_y'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}